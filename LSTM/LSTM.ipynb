{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM长短期记忆\n",
    "\n",
    "主要由四个函数构造的记忆细胞函数共同实现长短期记忆\n",
    "- 记忆细胞\n",
    "    - 遗忘门$F_t = \\sigma(X_tW_{xf} + H_{t-1}W_{hf} + b_f)$\n",
    "    - 输入门$I_t = \\sigma(X_tW_{xi} + H_{t-1}W_{hi} + b_i)$\n",
    "    - 候选记忆细胞$\\tilde{C_t} = \\tanh(X_tW_{xc} + H_{t-1}W_{hc} + b_c)$\n",
    "    - 输出门$O_t = \\sigma(X_tW_{xo} + H_{t-1}W_{ho} + b_o)$\n",
    "\n",
    "通过以下的两个公式，上面四个函数被组装起来：\n",
    "$$C_t = F_t \\odot C_{t-1} + I_t \\odot \\tilde{C_t}. \\tag{1}$$\n",
    "$$H_t = O_t \\odot \\tanh(C_t).  \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 公式分析\n",
    "\n",
    "分析以上的两个公式，可以发现：\n",
    "- 当前记忆细胞通过函数，协调了**当前时间点输入的数据**和**上一时间点的输入数据**两者对隐藏层输入的影响\n",
    "- 最后的隐藏层函数，则又对记忆细胞的记忆进行选择性保留（使用按位乘法）\n",
    "\n",
    "通过两层次的协调，在经过训练后对于**当前层数据**和**过去层数据**的信息利用更加有复杂的拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据导入和包导入\n",
    "import d2lzh as d2l\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import rnn\n",
    "(corpus_indices, char_to_idx, idx_to_char,\n",
    "vocab_size) = d2l.load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "ctx = d2l.try_gpu()\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n",
    "    def _three():\n",
    "        return (_one((num_inputs, num_hiddens)),\n",
    "                _one((num_hiddens, num_hiddens)),\n",
    "                nd.zeros(num_hiddens, ctx=ctx))\n",
    "    W_xi, W_hi, b_i = _three() # 输⼊⻔参数\n",
    "    W_xf, W_hf, b_f = _three() # 遗忘⻔参数\n",
    "    W_xo, W_ho, b_o = _three() # 输出⻔参数\n",
    "    W_xc, W_hc, b_c = _three() # 候选记忆细胞参数\n",
    "    \n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = nd.zeros(num_outputs, ctx=ctx)\n",
    "    # 附上梯度\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "    b_c, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    " def init_lstm_state(batch_size, num_hiddens, ctx):\n",
    "    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx),\n",
    "            nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "    W_hq, b_q] = params\n",
    "    \n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = nd.sigmoid(nd.dot(X, W_xi) + nd.dot(H, W_hi) + b_i)\n",
    "        F = nd.sigmoid(nd.dot(X, W_xf) + nd.dot(H, W_hf) + b_f)\n",
    "        O = nd.sigmoid(nd.dot(X, W_xo) + nd.dot(H, W_ho) + b_o)\n",
    "        C_tilda = nd.tanh(nd.dot(X, W_xc) + nd.dot(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * C.tanh()\n",
    "        Y = nd.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40, perplexity 219.243301, time 10.06 sec\n",
      " - 分开 我不的我 我不你的 我不的我 我不的你 我不的我 我不的你 我不的我 我不的你 我不的我 我不的你\n",
      " - 不分开 我不的你 我不的我 我不的你 我不的我 我不的你 我不的我 我不的你 我不的我 我不的你 我不的我\n",
      "epoch 80, perplexity 69.325723, time 10.79 sec\n",
      " - 分开 我想想你想你 我不要 我不 我不 我不要 我不不 我不不 我不不 我不不 我不不 我不不 我不不 \n",
      " - 不分开 我想想你想想 我不要 我不 我不 我不要 我不不 我不不 我不不 我不不 我不不 我不不 我不不 \n",
      "epoch 120, perplexity 16.331127, time 9.60 sec\n",
      " - 分开 我想你你看经 我想想你 你我的这面 你你去觉 我不要这生 我知知觉 我不了这节 后知后觉 我不了这\n",
      " - 不分开 我想要你 我不要 我不要这样 我不去觉 我爱了这节活 后知后觉 我该了这生活 后知后觉 我该了这生\n",
      "epoch 160, perplexity 4.301546, time 9.62 sec\n",
      " - 分开 你说你的你笑就  说 你想了了了我 说散 你想很久了吧? 我不想你远远久幽 想想大你 我我想多 我\n",
      " - 不分开 我已经这样奏我的手不能不开 爱能不能够永远不不没 不不要再想 我不能再想 我不 我不 我不能 爱情\n"
     ]
    }
   ],
   "source": [
    "# 训练和创作\n",
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n",
    "pred_period, pred_len, prefixes = 40, 50, ['分开', '不分开']\n",
    "\n",
    "d2l.train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,\n",
    "                            vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                            char_to_idx, False, num_epochs, num_steps, lr,\n",
    "                            clipping_theta, batch_size, pred_period, pred_len,\n",
    "                            prefixes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
