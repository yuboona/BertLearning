{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copyright (C) 2019 Yuboona Zhang\n",
    "#\n",
    "# LSTM for Punctuation Restoration in Speech Transcripts is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU Lesser General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# LSTM for Punctuation Restoration in Speech Transcripts is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Lesser General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU Lesser General Public License\n",
    "# along with LSTM for Punctuation Restoration in Speech Transcripts. If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入杰伦的歌词数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "import math\n",
    "from mxnet import autograd, nd\n",
    "from mxnet.gluon import loss as gloss\n",
    "import time\n",
    "\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1038\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onehot向量输入\n",
    "onehot向量可以考虑用词嵌入的向量来代替"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0. 1. 2. 3. 4.]\n",
      " [5. 6. 7. 8. 9.]]\n",
      "<NDArray 2x5 @cpu(0)>\n",
      "\n",
      "[[0. 5.]\n",
      " [1. 6.]\n",
      " [2. 7.]\n",
      " [3. 8.]\n",
      " [4. 9.]]\n",
      "<NDArray 5x2 @cpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\n",
       " [[1. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x1038 @cpu(0)>, \n",
       " [[0. 1. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x1038 @cpu(0)>, \n",
       " [[0. 0. 1. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x1038 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x1038 @cpu(0)>, \n",
       " [[0. 0. 0. ... 0. 0. 0.]\n",
       "  [0. 0. 0. ... 0. 0. 0.]]\n",
       " <NDArray 2x1038 @cpu(0)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_onehot(X, size):\n",
    "    return [nd.one_hot(x, size) for x in X.T]\n",
    "X = nd.arange(10).reshape((2, 5))\n",
    "print(X)   # [[0. 1. 2. 3. 4.]    一个batch的形状\n",
    "           # [5. 6. 7. 8. 9.]]\n",
    "print(X.T)\n",
    "inputs = to_onehot(X, vocab_size)\n",
    "inputs\n",
    "# len(inputs), inputs[0].shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will use cpu(0) to train\n"
     ]
    }
   ],
   "source": [
    "# num_hiddens是超参数\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "ctx = d2l.try_gpu()\n",
    "print(\"we will use\", ctx, \"to train\")\n",
    "\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n",
    "    \n",
    "    # 隐藏层参数\n",
    "    W_xh = _one((num_inputs, num_hiddens))\n",
    "    W_hh = _one((num_hiddens, num_hiddens))\n",
    "    b_h = nd.zeros(num_hiddens, ctx=ctx)\n",
    "    \n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = nd.zeros(num_outputs, ctx=ctx)\n",
    "    \n",
    "    #附上梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回隐藏状态的和输出\n",
    "def init_rnn_state(batch_size, num_hiddens, ctx):\n",
    "    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $H_t = \\phi(X_tW_{xh} + H_{t-1}W_{hh} + b_n)$\n",
    "- $O_t = H_tW_{hq} + b_q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs和outputs皆为num_steps个形状为(batch_size, vacab_size)的矩阵\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n",
    "        Y = nd.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试输出结果的个数，**第一个时间步的输出层输出的形状**和**隐藏状态的形状**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, (2, 1038), (2, 256))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = init_rnn_state(X.shape[0], num_hiddens, ctx)\n",
    "inputs = to_onehot(X.as_in_context(ctx), vocab_size)\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(inputs, state, params)\n",
    "len(outputs), outputs[0].shape, state_new[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义预测函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
    "               num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\n",
    "    state = init_rnn_state(1, num_hiddens, ctx)\n",
    "    outputs = [char_to_idx[prefix[0]]]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        # 将上一时间步的输出作为当前时间步的输入\n",
    "        X = to_onehot(nd.array([outputs[-1]], ctx=ctx), vocab_size)\n",
    "        # 计算输出和更新隐藏状态\n",
    "        (Y, state) = rnn(X, state, params)\n",
    "        if t < len(prefix) - 1:\n",
    "            outputs.append(char_to_idx[prefix[t + 1]])\n",
    "        else:\n",
    "            outputs.append(int(Y[0].argmax(axis=1).asscalar()))\n",
    "    return ''.join([idx_to_char[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下predict_run函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开身角酋带站这送爬斜世'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 裁剪梯度 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了避免梯度爆炸，要给梯度设定阈值，超过阈值就变1。\n",
    "\n",
    "公式 : $\\min(\\frac{\\theta}{||g||}, 1) \\times g$  这个公式表示，当g的二范数（$\\sqrt{g_1^2 + g_2^2}$）小于阈值$\\theta$时，g就保持不变，但大于$\\theta$时就会对g加上一个缩小乘子$\\frac{\\theta}{||g||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, ctx):\n",
    "    norm = nd.array([0], ctx)\n",
    "    for param in params:\n",
    "        norm += (param.grad **2).sum()\n",
    "    norm = norm.sqrt().asscalar()\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用困惑度(PPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "困惑度（PPL）是用来评价语言模型好坏的。困惑度是对交叉熵函数进行指数运算后得到的值。\n",
    "- 最佳，总是判断正确，分类预测正确概率1，困惑度1.\n",
    "- 最坏，总是预测错误，分类正确率0，困惑度正无穷\n",
    "- baseline，预测所有类别的概率都相同，困惑度为类别个数，即vocab_size\n",
    "\n",
    "有效的模型必须小于类别的个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型训练函数 \n",
    "与之前章节的模型训练函数相比，不同点：\n",
    "1. 使用困惑度评价模型\n",
    "2. 在 迭代 模型参数前 剪裁梯度\n",
    "3. 在时序数据采用不同采样方法 rnn中的状态初始化也要跟着不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
    "                         vocab_size, ctx, corpus_indices, idx_to_char,\n",
    "                         char_to_idx, is_random_iter, num_epochs, num_steps,\n",
    "                         lr, clipping_theta, batch_size, pred_period,\n",
    "                         pred_len, prefixes\n",
    "    \"\"\"\n",
    "    train_and_predict实际上是：\n",
    "    1. 进行batch分批\n",
    "    2. 创建epoch循环\n",
    "    3. 建立batch遍历，将所有batches遍历一遍\n",
    "    4. 对于一个batch，使用rnn函数进行前向计算，记录辅助w参数梯度下降计算所用到的其他梯度。然后使用反向传播计算梯度\n",
    "    \"\"\"\n",
    "    if is_random_iter:\n",
    "        data_iter_fn = d2l.data_iter_random\n",
    "    else:\n",
    "        data_iter_fn = d2l.data_iter_consecutive\n",
    "    params = get_params()\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if not is_random_iter:    # 相邻采样\n",
    "            state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)\n",
    "        for X, Y in data_iter:\n",
    "            if is_random_iter:\n",
    "                state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach()\n",
    "            with autograd.record():\n",
    "                # [[0. 5.]  经过to_onehot后的形状\n",
    "                # [1. 6.]\n",
    "                # [2. 7.]\n",
    "                # [3. 8.]\n",
    "                # [4. 9.]]\n",
    "                inputs = to_onehot(X, vocab_size)\n",
    "                # 一个inputs中包含的是时间步长个\n",
    "                outputs, state = rnn(inputs, state, params)\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                \n",
    "                y = Y.T.reshape((-1, ))\n",
    "                \n",
    "                l = loss(outputs, y).mean()\n",
    "            l.backward()\n",
    "            grad_clipping(params,clipping_theta, ctx)\n",
    "            d2l.sgd(params, lr, 1)\n",
    "            l_sum += l.asscalar() * y.size  # 直接转化为了对softmax运算的求和的\n",
    "            n += y.size\n",
    "            \n",
    "        if (epoch + 1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
    "            for prefix in prefixes:\n",
    "                print(' -', predict_rnn(\n",
    "                    prefix, pred_len, rnn, params, init_rnn_state,\n",
    "                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 69.552116, time 4.23 sec\n",
      " - 分开 我不要再想你 不知我不想你 不知我不 你不 我不要再想 我不要再想 我不要再想 我不要再想 我不要\n",
      " - 不分开 我不要再想你 不知我不想你 不知我不 你不 我不要再想 我不要再想 我不要再想 我不要再想 我不要\n",
      "epoch 100, perplexity 10.109862, time 1.98 sec\n",
      " - 分开 用一定美 快暖了外 恨果己空 恨我不起 却你说口 如果了空  我不要 静怎么的字活 我知你 你爱我\n",
      " - 不分开 爱要我 别怪我 想你怎么单！ 折这么(客  我爱道这里很久 我想要 想你眼睛看着我 泪开的我 你的\n",
      "epoch 150, perplexity 2.871562, time 4.60 sec\n",
      " - 分开 用色啦烛 温水了空屋人占卜 她说下午三点阳光射by堂 我的世界已狂的暴言 纪录第一次遇见的你（Ja\n",
      " - 不分开期 然伤将过的牛肉 我都店的有模有样 什么兵器最喜欢 双截棍柔中带染 想要你枪跟故 干什么(客) 瞎\n",
      "epoch 200, perplexity 1.605749, time 1.97 sec\n",
      " - 分开 用是心不的 静晶就什么抢了)在窝 那子风化千年的誓言 一切又重演 祭在西元前 爱在它不前 这蝪拽著\n",
      " - 不分开 已经 失去意义 戒指在哭泣 静静躺在抽屉 它所拥有的只剩下回忆 草原上 你时我 一九四三 在漠忆 \n",
      "epoch 250, perplexity 1.308990, time 4.20 sec\n",
      " - 分开 用我心口你 我听说你球在战争就就会换来和 只是我怕眼 让话开你已 有种不要走 三底三什么 懂不懂篮\n",
      " - 不分开 已经 失去意义 戒指在哭泣 静静躺在抽屉 它所拥有的只剩下回忆 还一定钟身时香香始摩远 一定的叹有\n"
     ]
    }
   ],
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2   # e-2 是 10^{-2}\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开'] \n",
    "\n",
    "train_and_predict_rnn(\n",
    "    rnn, get_params, init_rnn_state,\n",
    "    num_hiddens, vocab_size, ctx, corpus_indices,\n",
    "    idx_to_char, char_to_idx, True, num_epochs, \n",
    "    num_steps, lr, clipping_theta, batch_size, \n",
    "    pred_period, pred_len, prefixes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 小组件\n",
    "    1. to_onehot()输入数据变为onehot，大概是因为网络是由0，1激活态构成\n",
    "    2. get_params()获得初始化的参数\n",
    "    3. init_rnn_state()初始化rnn状态\n",
    "- 中组件，调用小组件\n",
    "    1. rnn()\n",
    "    2. predict_rnn()\n",
    "- 大组件，整合中组件和小组件\n",
    "    1. train_and_predict()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
